{
  "pipelineSpec": {
    "components": {
      "comp-train-model": {
        "executorLabel": "exec-train-model",
        "inputDefinitions": {
          "parameters": {
            "gcs_bucket": {
              "type": "STRING"
            },
            "model_name": {
              "type": "STRING"
            },
            "train_file_path": {
              "type": "STRING"
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-train-model": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "train_model"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-storage' 'pandas' 'scikit-learn==0.21.3' 'fsspec' 'gcsfs' 'kfp==1.8.19' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef train_model(gcs_bucket: str, train_file_path: str, model_name: str):\n    from google.cloud import storage\n\n    # gcs_bucket = \"ada_finance_dataset\"\n    train_pipeline_name = \"S&P_train_pipeline\"\n\n    # dataframe = pd.read_csv(f'gs://{gcs_bucket}/{train_file_path}')\n\n    output_file = f\"{train_pipeline_name}/artefacts/{model_name}\"\n\n    # TODO: GET STUFF FROM dataframe\n    # x_train = dataframe[0]\n    # y_train = dataframe[1]\n    # x_test = dataframe[2]\n    # y_test = dataframe[3]\n\n    # my_model = neural_net.train(x_train, y_train)\n\n    # y_pred = neural_net.test(my_model, x_test, y_test)\n\n    # print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\n    # my_model = lambda x : x+2\n\n    # joblib.dump(my_model, model_name)\n\n    bucket = storage.Client().bucket(gcs_bucket)\n    blob = bucket.blob(output_file)\n    blob.upload_from_filename(model_name)\n\n    print(f\"Model saved in : {output_file}\")\n\n"
            ],
            "image": "python:3.7"
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "trainingpipeline"
    },
    "root": {
      "dag": {
        "tasks": {
          "train-model": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-train-model"
            },
            "inputs": {
              "parameters": {
                "gcs_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ada_finance_dataset"
                    }
                  }
                },
                "model_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "hi"
                    }
                  }
                },
                "train_file_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "train-model"
            }
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.19"
  },
  "runtimeConfig": {
    "gcsOutputDirectory": "gs://ada_finance_dataset/S&P_train_pipeline"
  }
}